{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"04_Validation_and Fine_Tuning.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOka/C+DEwt16vc3RUnkGev"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NgxTwalluEYh","executionInfo":{"status":"ok","timestamp":1614786252929,"user_tz":360,"elapsed":17988,"user":{"displayName":"Alexey Pronin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFO04ue-bNrj83tCRld7Qz_P5Chn-2fMGkWtxLGQ=s64","userId":"07372911786440619972"}},"outputId":"8eece119-2c7e-4fad-8616-4aea891aa252"},"source":["# Mounting Google Drive\r\n","from google.colab import drive\r\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hAytMFvI3G4u"},"source":["## Validating a Machine Learning Model\r\n","\r\n","### Train-test split\r\n","\r\n","We have already seen that when assessing the quality of a machine learning model (e.g., linear regression) it is usually not the best idea to use the same data as the ones that were used in the process of fitting (or *training*) the model. A fair assessment can only be obtain if the model is presented data it has never seen before. But where are we supposed to obtain such data if we have only one data set (e.g., Auto MPG)? Usually, we cannot just create new data on the fly -- the new data must be collected and the data collection process is probably the most expensive and time-consuming part of any data scientist job. So, what do we do if we are only limited to the data set at hand? One popular approach is to split the data set you are given into two chunks: \r\n","  * a larger chunk including about 60-95% of the data which we will be used for training; it is called a *training set*;\r\n","  * a smaller chunk including about 5-40% of the data which will be used for assessing the quality of the model; it is called a *test* or *holdout set*.\r\n","\r\n","  <img src='https://drive.google.com/uc?export=view&id=1sN5LobAApdl3pwqZBlioDNwnYlbkTsaI' width='800' align=\"center\"/>\r\n","\r\n","The idea here is that we must never use the test set in the process of fitting our model -- the model is not supposed to see these data during the training phase. Then checking the quality of the model on the test set will give us a much better estimate of the actual performance of the model. This should be done only at the very last stage of our work, when we are ready to deploy the model.\r\n","\r\n","In sklearn, we can easily create a split into the training set and test set using a special function called `train_test_split()`. This function takes your features and labels as an input and spits out the splitted versions of those. Here is an example illustrating how this function works on the Auto MPG data (we are only using the car MPG and weight data):"]},{"cell_type":"code","metadata":{"id":"c-BOKzgTUlmy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614786256794,"user_tz":360,"elapsed":21843,"user":{"displayName":"Alexey Pronin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFO04ue-bNrj83tCRld7Qz_P5Chn-2fMGkWtxLGQ=s64","userId":"07372911786440619972"}},"outputId":"ea8dad5f-b623-46cd-b203-5bebd23010ed"},"source":["import numpy as np\r\n","from pathlib import Path\r\n","# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\r\n","from sklearn.model_selection import train_test_split\r\n","\r\n","PATH = Path('/content/gdrive/My Drive/Colab Notebooks/Applied_Machine_Learning/Data/Auto_MPG')\r\n","\r\n","weight=np.load(PATH/'weight.npy')\r\n","mpg=np.load(PATH/'mpg.npy')\r\n","\r\n","X = weight.reshape(-1, 1)\r\n","y = mpg\r\n","\r\n","print(f\"The shape of the 'weight' data is {X.shape}.\")\r\n","print(f\"The shape of the 'mpg' data is {y.shape}.\")\r\n","\r\n","# By default, 'train_test_split' will perform random shuffling.\r\n","# To get a reproducible split, set the value of the 'random_state' parameter.\r\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1442)\r\n","\r\n","print(f\"The shape of X_train is {X_train.shape}.\")\r\n","print(f\"The shape of y_train is {y_train.shape}.\")\r\n","print(f\"The shape of X_test is {X_test.shape}.\")\r\n","print(f\"The shape of y_test is {y_test.shape}.\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["The shape of the 'weight' data is (398, 1).\n","The shape of the 'mpg' data is (398,).\n","The shape of X_train is (278, 1).\n","The shape of y_train is (278,).\n","The shape of X_test is (120, 1).\n","The shape of y_test is (120,).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"irnzl__QOJ8O"},"source":["### K-fold cross-validation\r\n","\r\n","Once we have our data split into the test set and training set, we become ready to fit our model on the training data. But what model are we supposed to use? We know that the relationship between our label (MPG) and feature (weight) is non-linear but we don't really know what power would work best. We also do not know what set of hyperparameters (e.g., the learning rate $\\eta$, regularization term coefficient $\\alpha$) would be optimal. \r\n","\r\n","To address all these questions we need to run a lot of experiments with our training data trying various polynomial models with different hyperparameters. To decide which one of these experiments yields the best result we need to be able to compare their outcomes, for example, by computing and comparing the resulting $\\text{RMSE}$ scores (the metric). But here we have the same problem as before: to be able to make an objective comarisson we need to compute the $\\text{RMSE}$ scores on data that have never been shown to the model during its training phase. We cannot use the test set data -- those are supposed to be used only at the very last stage when we are ready to deploy our model. So what do we do?\r\n","\r\n","One solution would be to use a chunk of our training data for the on-going quality assessment of different models. This chunk is typically called the *validation set* or *development set* and it can be created using the same `train_test_split()` function as before. \r\n","\r\n","  <img src='https://drive.google.com/uc?export=view&id=1sQc3ojQj4WhZvQ6efuRR5yk2O-0xNa1E' width='800' align=\"center\"/>\r\n","\r\n","You may have noticed that this validation set method does have some downsides. For example, in order for this method to work, we need to exclude some part of the data from the training process. Thus, the model is going to be exposed to a smaller amount of data during the training phase and, as a result, may demonstrate a worse performance. We have a dilemma: on one hand, if the data set is split into the training and validation sets then we can get an objective assessment of the performance of our models; but then the performance of the models may get worse; on the other hand, if we do not do the split then we are loosing a way to objectively compare performances of different models. So, what do we do? One popular solution to this dilemma is to use a *K-fold cross-validation* method in place of the train-test split method. \r\n","\r\n","The idea of K-fold cross-validation is illustrated in the picture below (with $K=5$). \r\n","\r\n","<img src='https://drive.google.com/uc?export=view&id=1sTHRRLnZzX6dGwM2xxLwJByA7ANJnpNx' width='600' align=\"center\"/>\r\n","\r\n","We split the training set into $K$ non-overlapping folds (a very popular choice is $K=5$). And then we train our model not one but $K$ times: one time for each fold; every time we use the current fold data for validation and the rest of the data for training. Essentially, at the end of this process, we end up with $K$ slightly different models, trained on slightly different data. The overall metric is computed as an average of the $K$ metric values computed for individual folds and, when making predictions on new data, we again just average the individual prediction of the $K$ models. Note that during the training process all training data are used (though not simultaneously), so we can expect a better performance that the one we get with a simple validation set approach.\r\n","\r\n","A great news is that sklearn can automate this process for us with the help of a special `cross_val_score()` function. Here is an example illustrating the usage of this function (we are using the same car MPG and car weight data as before)."]},{"cell_type":"code","metadata":{"id":"50goowD1UGMv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614786256939,"user_tz":360,"elapsed":21981,"user":{"displayName":"Alexey Pronin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFO04ue-bNrj83tCRld7Qz_P5Chn-2fMGkWtxLGQ=s64","userId":"07372911786440619972"}},"outputId":"587a10b7-18cc-4bf2-9a1d-0bed2cf13c94"},"source":["# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\r\n","from sklearn.model_selection import cross_val_score\r\n","from sklearn.pipeline import Pipeline\r\n","from sklearn.preprocessing import StandardScaler, PolynomialFeatures\r\n","from sklearn.linear_model import Lasso\r\n","\r\n","n_folds=5\r\n","poly_degree=2\r\n","alpha=0.05\r\n","\r\n","cv_pipe = Pipeline([                    \r\n","                    ('poly', PolynomialFeatures(degree=poly_degree, include_bias=False)),\r\n","                    ('scaler', StandardScaler()),\r\n","                    ('lasso', Lasso(alpha=alpha))\r\n","                    ])\r\n","\r\n","score = cross_val_score(cv_pipe, X=X_train, y=y_train, cv=n_folds, scoring='neg_root_mean_squared_error')\r\n","\r\n","print(f\"The RMSE scores for the folds:\\n {-score}\\n\")\r\n","print(f\"The average over {n_folds} folds RMSE score is {np.mean(-score):.3f}.\")"],"execution_count":3,"outputs":[{"output_type":"stream","text":["The RMSE scores for the folds:\n"," [4.54200692 4.77359256 3.58901793 4.54451504 4.38312039]\n","\n","The average over 5 folds RMSE score is 4.366.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kKWT9HnzC0lC"},"source":["Another popular regression metric is the $R^2$ score, or the *coefficient of determination* which is a measure of how well our model performs relative to a simple mean of the target values. The highest and the best value for $R^2$ is $1$ -- it means that the model reproduces all data points perfectly; $R^2=0$ means that the model performs not better that a simple avearge of the target variable; $R^2 < 0$ means that the model performs even worse than a simple average."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zr9KgbP0E8XM","executionInfo":{"status":"ok","timestamp":1614786257085,"user_tz":360,"elapsed":22122,"user":{"displayName":"Alexey Pronin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFO04ue-bNrj83tCRld7Qz_P5Chn-2fMGkWtxLGQ=s64","userId":"07372911786440619972"}},"outputId":"b2313a7f-da74-4991-cecf-d49b21fd0281"},"source":["score_r2 = cross_val_score(cv_pipe, X=X_train, y=y_train, cv=n_folds, scoring='r2')\r\n","\r\n","print(f\"The R^2 scores for the folds:\\n {score_r2}\\n\")\r\n","print(f\"The average over {n_folds} folds R^2 score is {np.mean(score_r2):.3f}.\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["The R^2 scores for the folds:\n"," [0.69329691 0.69215281 0.74066015 0.72801033 0.65797403]\n","\n","The average over 5 folds R^2 score is 0.702.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jRIkkh9w3nRc"},"source":["### Grid search\r\n","\r\n","Sklearn provides a convenient way to search for the best parameters of a model using `GridSearchCV()` class. Here is an example:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k_uaIDWLrUqo","executionInfo":{"status":"ok","timestamp":1614786257491,"user_tz":360,"elapsed":22520,"user":{"displayName":"Alexey Pronin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFO04ue-bNrj83tCRld7Qz_P5Chn-2fMGkWtxLGQ=s64","userId":"07372911786440619972"}},"outputId":"d4174e7e-686d-48e6-ab11-7c92cb788a14"},"source":["# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\r\n","from sklearn.model_selection import GridSearchCV\r\n","\r\n","# make lists of different parameters to check\r\n","parameters = [\r\n","              {'poly__degree':[2, 3, 4], 'lasso__alpha': [0.5, 0.05, 0.005]},\r\n","              {'poly__degree':[2, 3, 4]}\r\n","              ]\r\n","# Instantiate\r\n","grid_pipeline = GridSearchCV(cv_pipe, parameters, cv=n_folds, scoring='neg_root_mean_squared_error')\r\n","# Fit\r\n","grid_pipeline.fit(X_train,y_train)\r\n","# Show results\r\n","grid_pipeline.cv_results_"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.04367864565711, tolerance: 1.4898354170403587\n","  positive)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 32.82796465639876, tolerance: 1.425937211711712\n","  positive)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.375957804428253, tolerance: 1.4898354170403587\n","  positive)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["{'mean_fit_time': array([0.00195899, 0.00244994, 0.00146036, 0.00146465, 0.00142345,\n","        0.00123806, 0.00133982, 0.00178108, 0.00318127, 0.00184565,\n","        0.0026032 , 0.0022202 ]),\n"," 'mean_score_time': array([0.00072508, 0.00090504, 0.00061727, 0.00057302, 0.00054326,\n","        0.00052156, 0.00052853, 0.00055895, 0.00076571, 0.00080943,\n","        0.00108638, 0.00095644]),\n"," 'mean_test_score': array([-4.53478109, -4.53478109, -4.53478109, -4.36645057, -4.31468616,\n","        -4.30832062, -4.29836053, -4.29753114, -4.30284297, -4.36645057,\n","        -4.31468616, -4.30832062]),\n"," 'param_lasso__alpha': masked_array(data=[0.5, 0.5, 0.5, 0.05, 0.05, 0.05, 0.005, 0.005, 0.005,\n","                    --, --, --],\n","              mask=[False, False, False, False, False, False, False, False,\n","                    False,  True,  True,  True],\n","        fill_value='?',\n","             dtype=object),\n"," 'param_poly__degree': masked_array(data=[2, 3, 4, 2, 3, 4, 2, 3, 4, 2, 3, 4],\n","              mask=[False, False, False, False, False, False, False, False,\n","                    False, False, False, False],\n","        fill_value='?',\n","             dtype=object),\n"," 'params': [{'lasso__alpha': 0.5, 'poly__degree': 2},\n","  {'lasso__alpha': 0.5, 'poly__degree': 3},\n","  {'lasso__alpha': 0.5, 'poly__degree': 4},\n","  {'lasso__alpha': 0.05, 'poly__degree': 2},\n","  {'lasso__alpha': 0.05, 'poly__degree': 3},\n","  {'lasso__alpha': 0.05, 'poly__degree': 4},\n","  {'lasso__alpha': 0.005, 'poly__degree': 2},\n","  {'lasso__alpha': 0.005, 'poly__degree': 3},\n","  {'lasso__alpha': 0.005, 'poly__degree': 4},\n","  {'poly__degree': 2},\n","  {'poly__degree': 3},\n","  {'poly__degree': 4}],\n"," 'rank_test_score': array([10, 10, 10,  8,  6,  4,  2,  1,  3,  8,  6,  4], dtype=int32),\n"," 'split0_test_score': array([-4.73154008, -4.73154008, -4.73154008, -4.54200692, -4.48433143,\n","        -4.47134746, -4.46498168, -4.4532457 , -4.45293395, -4.54200692,\n","        -4.48433143, -4.47134746]),\n"," 'split1_test_score': array([-4.96817383, -4.96817383, -4.96817383, -4.77359256, -4.70230962,\n","        -4.69405825, -4.67260158, -4.67389933, -4.67857296, -4.77359256,\n","        -4.70230962, -4.69405825]),\n"," 'split2_test_score': array([-3.69167726, -3.69167726, -3.69167726, -3.58901793, -3.5423415 ,\n","        -3.54903753, -3.51587653, -3.53116176, -3.54536834, -3.58901793,\n","        -3.5423415 , -3.54903753]),\n"," 'split3_test_score': array([-4.81295152, -4.81295152, -4.81295152, -4.54451504, -4.44371199,\n","        -4.42859318, -4.37630944, -4.38013224, -4.38742311, -4.54451504,\n","        -4.44371199, -4.42859318]),\n"," 'split4_test_score': array([-4.46956276, -4.46956276, -4.46956276, -4.38312039, -4.40073624,\n","        -4.39856668, -4.46203341, -4.44921669, -4.44991652, -4.38312039,\n","        -4.40073624, -4.39856668]),\n"," 'std_fit_time': array([4.51239914e-04, 5.17668426e-04, 2.03889870e-04, 2.15973936e-04,\n","        2.27652697e-04, 3.12659038e-05, 2.42281143e-05, 5.28163025e-04,\n","        8.12194317e-04, 4.48478806e-04, 2.96267242e-04, 1.53421968e-04]),\n"," 'std_score_time': array([4.07313285e-05, 1.75124638e-04, 9.07624177e-05, 8.53395450e-05,\n","        4.26095025e-05, 2.30508231e-05, 7.50280805e-05, 8.93235262e-05,\n","        2.11496583e-04, 3.10784631e-04, 1.55902388e-04, 1.45123100e-05]),\n"," 'std_test_score': array([0.45144206, 0.45144206, 0.45144206, 0.40815952, 0.39989999,\n","        0.39357132, 0.40322823, 0.3957407 , 0.39147224, 0.40815952,\n","        0.39989999, 0.39357132])}"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TOCKTUKa4vRU","executionInfo":{"status":"ok","timestamp":1614786257492,"user_tz":360,"elapsed":22513,"user":{"displayName":"Alexey Pronin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFO04ue-bNrj83tCRld7Qz_P5Chn-2fMGkWtxLGQ=s64","userId":"07372911786440619972"}},"outputId":"fe6cf6c3-b342-4338-8078-70f027f92988"},"source":["# Printing the best parameters\r\n","grid_pipeline.best_params_"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'lasso__alpha': 0.005, 'poly__degree': 3}"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"nwqOHfju73s7","executionInfo":{"status":"ok","timestamp":1614786257493,"user_tz":360,"elapsed":22512,"user":{"displayName":"Alexey Pronin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFO04ue-bNrj83tCRld7Qz_P5Chn-2fMGkWtxLGQ=s64","userId":"07372911786440619972"}}},"source":["# Call predict on the estimator with the best found parameters\r\n","y_pred=grid_pipeline.predict(X_train)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CKesz3bq7_Ez","executionInfo":{"status":"ok","timestamp":1614786257493,"user_tz":360,"elapsed":22506,"user":{"displayName":"Alexey Pronin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFO04ue-bNrj83tCRld7Qz_P5Chn-2fMGkWtxLGQ=s64","userId":"07372911786440619972"}},"outputId":"7340c3c9-2408-4790-bb82-6d1cf07466e6"},"source":["from sklearn.metrics import mean_squared_error, r2_score\r\n","\r\n","grid_rmse=mean_squared_error(y_train, y_pred)**0.5\r\n","print(f\"The RMSE score after grid search CV is {grid_rmse:.3f}\")\r\n","\r\n","grid_r2=r2_score(y_train, y_pred)\r\n","print(f\"The R^2 score after grid search CV is {grid_r2:.3f}\")"],"execution_count":8,"outputs":[{"output_type":"stream","text":["The RMSE score after grid search CV is 4.272\n","The R^2 score after grid search CV is 0.719\n"],"name":"stdout"}]}]}